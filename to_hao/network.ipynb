{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "COLUMNS = [\"media_type\", \"mode_name\", \"print_height\",\n",
    "           \"print_width\",\"print_id\", \"R\", \"G\", \"B\",\"width\",\"height\",\"w/h\",\"size\",\"print_time_sec\"]\n",
    "FEATURES = [\"media_type\", \"media_waste\", \"mode_name\", \"print_height\",\n",
    "           \"print_width\",\"print_id\", \"R\", \"G\", \"B\",\"width\",\"height\",\"w/h\",\"size\"]\n",
    "LABEL = \"print_time_sec\"\n",
    "\n",
    "data_set = pd.read_csv(\"data_sheet.csv\", skipinitialspace=True,\n",
    "                           skiprows=1, names=COLUMNS).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_fields = ['media_type', 'mode_name','print_id']\n",
    "for each in dummy_fields:\n",
    "    dummies = pd.get_dummies(data_set[each], prefix=each, drop_first=False)\n",
    "    data_set = pd.concat([data_set, dummies], axis=1)\n",
    "    \n",
    "fields_to_drop = ['media_type', 'mode_name','print_id']\n",
    "data = data_set.drop(fields_to_drop, axis=1)\n",
    "data.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21672, 24)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time = data_set['print_time_sec']\n",
    "data = data.drop('print_time_sec', axis=1)\n",
    "data = pd.concat([data, time], axis=1)\n",
    "data.to_csv('out.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEATURES =['print_height', 'print_width', 'R', 'G', 'B', 'width', 'height',\n",
    "       'w/h', 'size', 'media_type_1.0', 'media_type_2.0',\n",
    "       'media_type_3.0', 'media_type_4.0', 'media_type_5.0',\n",
    "       'mode_name_1.0', 'mode_name_2.0', 'mode_name_3.0', 'mode_name_4.0',\n",
    "       'print_id_1.0', 'print_id_2.0', 'print_id_3.0', 'print_id_4.0',\n",
    "       'print_id_5.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cut data to three sets\n",
    "def train_split(data):\n",
    "    data = shuffle(data)\n",
    "    total=data.shape[0]\n",
    "    # split data  to test data 5%\n",
    "    test = data[-int(0.05*total):]\n",
    "    # Now remove the test data from the data set \n",
    "    data = data[:-int(0.05*total)]\n",
    "    # split data  to valid data 2%\n",
    "    valid = data[-int(0.01*total):]\n",
    "    # Now remove the test data from the data set \n",
    "    data = data[:-int(0.01*total)]\n",
    "    \n",
    "    return data,test,valid\n",
    "\n",
    "train,test,valid=train_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216, 24) (1083, 24) (20373, 24)\n"
     ]
    }
   ],
   "source": [
    "print (valid.shape,test.shape,train.shape)\n",
    "train.to_csv('train.csv')\n",
    "test.to_csv('test.csv')\n",
    "valid.to_csv('valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_fn(data_set):\n",
    "    #data_set = (data_set - data_set.mean()) / (data_set.max() - data_set.min())\n",
    "    data_set = (data_set - data_set.mean()) / data_set.std()\n",
    "    feature_cols = data_set[FEATURES]\n",
    "    labels = data_set[LABEL]\n",
    "    return feature_cols, labels\n",
    "def test_fn(data_set):\n",
    "    feature_cols = data_set[FEATURES]\n",
    "    labels = data_set[LABEL]\n",
    "    return feature_cols, labels\n",
    "def valid_fn(data_set):\n",
    "    feature_cols = data_set[FEATURES]\n",
    "    labels = data_set[LABEL]\n",
    "    return feature_cols, labels\n",
    "\n",
    "train_x, train_y = train_fn(train)\n",
    "test_x, test_y = test_fn(test)\n",
    "valid_x, valid_y = valid_fn(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "training_iters = 50\n",
    "batch_size = 150\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 23 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 1 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "model_path = \"network/my-model\"\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_input]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32) # 0-9 digits recognition => 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x=train_x.values\n",
    "train_y=train_y.values\n",
    "test_x=test_x.values\n",
    "test_y=test_y.values\n",
    "valid_x=valid_x.values\n",
    "valid_y=valid_y.values\n",
    "train_x = tf.constant(train_x)\n",
    "train_y = tf.constant(train_y)\n",
    "test_x = tf.constant(test_x)\n",
    "test_y = tf.constant(test_y)\n",
    "valid_x = tf.constant(valid_x)\n",
    "valid_y = tf.constant(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def network(x, weights, biases, dropout):\n",
    "    \n",
    "    # Fully connected layer\n",
    "    fc1 = tf.add(tf.matmul(x, weights['wc1']), biases['bc1'])\n",
    "    #fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    #fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    \n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wc2']), biases['bc2'])\n",
    "    #fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([n_input, 10])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([10, 10])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([10, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([10])),\n",
    "    'bc2': tf.Variable(tf.random_normal([10])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = network(x, weights, biases, dropout)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.losses.mean_squared_error(predictions=pred, labels=y))\n",
    "\n",
    "# Evaluate model\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) \n",
    "\n",
    "# Predictions for the validation, and test data.\n",
    "\n",
    "test_loss = tf.reduce_mean(tf.losses.mean_squared_error(predictions=pred, labels=y))\n",
    "\n",
    "valid_pred = network(x, weights, biases, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "INFO:tensorflow:Restoring parameters from network/my-model-50\n",
      "Loss at step 0: 1364609.750000\n",
      "Loss at step 10: 39224.867188\n",
      "Loss at step 20: 15619.988281\n",
      "Loss at step 30: 12846.159180\n",
      "Loss at step 40: 1057.232422\n",
      "global_step: 50\n",
      "runtime: 0:07:44.229830\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  print(\"Initialized\")\n",
    "  saver = tf.train.import_meta_graph('network/my-model.meta')\n",
    "  saver.restore(sess, tf.train.latest_checkpoint('network/'))   \n",
    "  #print(\"Model restored from file: %s\" % save_path)\n",
    "  # get current time\n",
    "  t1 = datetime.datetime.now()\n",
    "  for step in range(training_iters):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) %(int(train_x.shape[0]) - batch_size)\n",
    "    # Generate a minibatch. \n",
    "    batch_data = train_x[offset:(offset + batch_size)].eval()\n",
    "    batch_labels = train_y[offset:(offset + batch_size)].eval()\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {x : batch_data, y : batch_labels}\n",
    "    _, l = sess.run([optimizer, cost], feed_dict=feed_dict)\n",
    "    if (step % 10 == 0):\n",
    "      print(\"Loss at step %d: %f\" % (step, l))\n",
    "  global_step+=training_iters \n",
    "  print('global_step: %s' % tf.train.global_step(sess, global_step))\n",
    "  t2 = datetime.datetime.now()\n",
    "  # print runtime\n",
    "  print ('runtime:',t2-t1)\n",
    "  # Save model weights to disk\n",
    "  save_path = saver.save(sess, model_path,global_step=global_step)\n",
    "  saver.export_meta_graph(model_path + '.meta') \n",
    "  print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  print(\"Initialized\")\n",
    "  saver = tf.train.import_meta_graph('network/my-model.meta')\n",
    "  saver.restore(sess, tf.train.latest_checkpoint('network/'))   \n",
    "  print(\"Test loss %f\",sess.run([test_loss], feed_dict={x:test_x[:].eval(), y:test_y[:].eval()}))\n",
    "  #for step in range(int(valid_x.shape[0])):\n",
    "  pred = sess.run([valid_pred], feed_dict={x:valid_x[:].eval()})\n",
    "  print(\"Prediction: %d\",(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred=np.asarray(pred)\n",
    "pred = pred.reshape(216,1)\n",
    "pred.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
