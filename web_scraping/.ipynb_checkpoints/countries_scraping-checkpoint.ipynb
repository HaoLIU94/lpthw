{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import re\n",
    "import lxml.html\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download(url,user_agent='wswp',num_retries=2,scrape_callback=None):\n",
    "    print 'Downloading:', url\n",
    "    headers = {'User-agent': user_agent}\n",
    "    request = urllib2.Request(url, headers=headers)\n",
    "    try:\n",
    "        html = urllib2.urlopen(url).read()\n",
    "    except urllib2.URLError as e:\n",
    "        print 'Download error:', e.reason\n",
    "        html = None\n",
    "        if num_retries > 0:\n",
    "            if hasattr(e, 'code') and 500 <= e.code < 600:\n",
    "                # recursively retry 5xx HTTP errors\n",
    "                return download(url,user_agent ,num_retries-1)\n",
    "    links = []\n",
    "    if scrape_callback:\n",
    "        links.extend(scrape_callback(url, html) or [])\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_links(html):\n",
    "    \"\"\"Return a list of links from html\n",
    "    \"\"\"\n",
    "    # a regular expression to extract all links from the webpage\n",
    "    webpage_regex = re.compile('<a[^>]+href=[\"\\'](.*?)[\"\\']',\n",
    "    re.IGNORECASE)\n",
    "    # list of all links from the webpage\n",
    "    return webpage_regex.findall(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_crawler(seed_url, link_regex,scrape_callback=None):\n",
    "    crawl_queue = [seed_url]\n",
    "    # keep track which URL's have seen before\n",
    "    seen = set(crawl_queue)\n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        html = download(url)\n",
    "        for link in get_links(html):\n",
    "            # check if link matches expected regex\n",
    "            if re.match(link_regex, link):\n",
    "                # form absolute link\n",
    "                link = urllib2.urlparse.urljoin(seed_url, link)\n",
    "                # check if have already seen this link\n",
    "                if link not in seen:\n",
    "                    seen.add(link)\n",
    "                    crawl_queue.append(link)\n",
    "    links = []\n",
    "    if scrape_callback:\n",
    "        links.extend(scrape_callback(url, html) or [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.webscraping.com\n"
     ]
    }
   ],
   "source": [
    "url = 'http://example.webscraping.com'\n",
    "html=download(url)\n",
    "tree = lxml.html.fromstring(html)\n",
    "td = tree.cssselect('tr > td > div> a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' Afghanistan': '/places/default/view/Afghanistan-1',\n",
       " ' Aland Islands': '/places/default/view/Aland-Islands-2',\n",
       " ' Albania': '/places/default/view/Albania-3',\n",
       " ' Algeria': '/places/default/view/Algeria-4',\n",
       " ' American Samoa': '/places/default/view/American-Samoa-5',\n",
       " ' Andorra': '/places/default/view/Andorra-6',\n",
       " ' Angola': '/places/default/view/Angola-7',\n",
       " ' Anguilla': '/places/default/view/Anguilla-8',\n",
       " ' Antarctica': '/places/default/view/Antarctica-9',\n",
       " ' Antigua and Barbuda': '/places/default/view/Antigua-and-Barbuda-10'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {}\n",
    "for p in td:\n",
    "    data[p.text_content()]=p.get(\"href\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.webscraping.com/places/default/view/Aland-Islands-2\n",
      "Downloading: http://example.webscraping.com/places/default/view/Angola-7\n",
      "Downloading: http://example.webscraping.com/places/default/view/Afghanistan-1\n",
      "Downloading: http://example.webscraping.com/places/default/view/Antigua-and-Barbuda-10\n",
      "Downloading: http://example.webscraping.com/places/default/view/Algeria-4\n",
      "Downloading: http://example.webscraping.com/places/default/view/American-Samoa-5\n",
      "Downloading: http://example.webscraping.com/places/default/view/Antarctica-9\n",
      "Downloading: http://example.webscraping.com/places/default/view/Anguilla-8\n",
      "Downloading: http://example.webscraping.com/places/default/view/Albania-3\n",
      "Downloading: http://example.webscraping.com/places/default/view/Andorra-6\n"
     ]
    }
   ],
   "source": [
    "for keys,values in data.items():\n",
    "    #link_crawler(url, values, scrape_callback=ScrapeCallback())\n",
    "    link = urllib2.urlparse.urljoin(url, values)\n",
    "    html = download(link)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "class ScrapeCallback:\n",
    "    def __init__(self):\n",
    "        self.writer = csv.writer(open('countries.csv', 'a'))\n",
    "        self.fields = ('area', 'population', 'iso', 'country',\n",
    "        'capital', 'continent', 'tld', 'currency_code',\n",
    "        'currency_name', 'phone', 'postal_code_format',\n",
    "        'postal_code_regex', 'languages',\n",
    "        'neighbours')\n",
    "        self.writer.writerow(self.fields)\n",
    "    def __call__(self, url, html):\n",
    "        if re.search('/view/', url):\n",
    "            tree = lxml.html.fromstring(html)\n",
    "            row = []\n",
    "            for field in self.fields:\n",
    "                row.append(tree.cssselect('table > tr#places_{}__row > td.w2p_fw'.format(field))[0].text_content())\n",
    "        self.writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compare among re & BeautifulSoup & lxml\n",
    "FIELDS = ['area', 'population', 'iso', 'country', 'capital',\n",
    "'continent', 'tld', 'currency_code', 'currency_name', 'phone',\n",
    "'postal_code_format', 'postal_code_regex', 'languages',\n",
    "'neighbours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxml.html\n",
    "def lxml_scraper(html):\n",
    "    tree = lxml.html.fromstring(html)\n",
    "    results = {}\n",
    "    for field in FIELDS:\n",
    "        results[field] = tree.cssselect('table > tr#places_%s__row > td.w2p_fw' % field)[0].text_content()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.webscraping.com/places/default/view/Aland-Islands-2\n",
      "Downloading: http://example.webscraping.com/places/default/view/Angola-7\n",
      "Downloading: http://example.webscraping.com/places/default/view/Afghanistan-1\n",
      "Downloading: http://example.webscraping.com/places/default/view/Antigua-and-Barbuda-10\n",
      "Downloading: http://example.webscraping.com/places/default/view/Algeria-4\n",
      "Downloading: http://example.webscraping.com/places/default/view/American-Samoa-5\n",
      "Downloading: http://example.webscraping.com/places/default/view/Antarctica-9\n",
      "Downloading: http://example.webscraping.com/places/default/view/Anguilla-8\n",
      "Downloading: http://example.webscraping.com/places/default/view/Albania-3\n",
      "Downloading: http://example.webscraping.com/places/default/view/Andorra-6\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for keys,values in data.items():\n",
    "    link = urllib2.urlparse.urljoin(url, values)\n",
    "    html = download(link,scrape_callback=ScrapeCallback())\n",
    "    res.append(re_scraper(html))\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "keys = res[0].keys()\n",
    "with open('countries.csv', 'wb') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'languages': 'sv-AX', 'area': '1,580 square kilometres', 'country': 'Aland Islands', 'postal_code_regex': '^(?:FI)*(\\\\d{5})$', 'tld': '.ax', 'currency_name': 'Euro', 'phone': '+358-18', 'neighbours': '<div><a href=\"/places/default/iso//\"> </a></div>', 'iso': 'AX', 'postal_code_format': '#####', 'capital': 'Mariehamn', 'continent': '<a href=\"/places/default/continent/EU\">EU</a>', 'currency_code': 'EUR', 'population': '26,711'}\n"
     ]
    }
   ],
   "source": [
    "print res[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
